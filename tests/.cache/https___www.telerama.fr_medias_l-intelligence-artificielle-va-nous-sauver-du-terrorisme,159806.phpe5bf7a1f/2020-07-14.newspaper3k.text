En psychologie, on appelle ça le sentiment de déjà-vu. En antiterrorisme, c’est simplement la routine. Une nouvelle attaque à Londres ? Theresa May s’en va-t-en guerre contre Internet et son pouvoir toxique. Un attentat manqué sur les Champs-Elysées ? Gérard Collomb, notre nouveau ministre de l’Intérieur, y voit la raison d’être d’un état d’urgence qui n’a pourtant rien vu venir. Et nous ? On rame, on ânonne, on (se) répète. Depuis de longs mois, nos gouvernements, obsédés par la radicalisation-en-ligne, semblent avoir fait un choix : plutôt que de s’attaquer aux causes, ils préfèrent traiter les effets. Par impuissance ou par opportunisme politique - peut-être un peu des deux -, ils en sont réduits à empiler les mesures d’affichage, comme autant d’onguents appliqués sur la peau meurtrie d’“opinions publiques” bien pratiques. Et tant pis si ce sont des placebos.

Le 14 juin, Theresa May et Emmanuel Macron ont dévoilé leur “plan d’action franco-britannique” pour juguler “l’utilisation de l’Internet à des fins terroristes”. Dans une France qui a voté 25 lois sécuritaires depuis 1986 (près d’un texte par an), on se demande bien pourquoi personne n’y avait pensé avant. Tout ira mieux désormais, c’est certain. Ainsi, la Première ministre et le président de la République ont identifié cinq axes de travail : une modération automatique des contenus par les réseaux sociaux ; la rétention des données (sur le modèle de la récente loi renseignement votée outre-Manche, qui oblige les fournisseurs d’accès à conserver l’historique de navigation de leurs clients pendant un an) ; un coup de canif dans le chiffrement des communications, sans pour autant installer de backdoors (bon courage avec ce serpent de mer !) ; un accès aux données stockées par les GAFA aux Etats-Unis ; et une pièce dans la machine du “contre-discours”, pour lutter pied à pied avec la propagande djihadiste.

Après cette salve d’annonces, il n’a pas fallu 24 heures à Facebook pour en prendre acte dans un long argumentaire plein de bonnes résolutions. L’occasion de se rendre compte que la firme de Mark Zuckerberg dispose désormais d’un “Counterterrorism Policy Manager” en la personne de Brian Fishman. Fidèle à sa doctrine “solutionniste”, le réseau social veut jouer son rôle de délégué aux affaires régaliennes et assure “qu’il n’y a pas de place pour le terrorisme sur Facebook”. L’arme fatale : l’intelligence artificielle. Jusqu’ici, les techniques d’apprentissage profond de la firme de Menlo Park se concentraient surtout sur l’identification et la compréhension d’images inoffensives, publiées par millions chaque jour sur la plateforme. Désormais, finis les bébés phoques, bonjour les drapeaux noirs de l'Etat islamique : l’IA servira aussi à débusquer les contenus menaçants. “Même si notre utilisation de l’IA pour lutter contre le terrorisme est assez récente, elle change déjà la façon dont nous empêchons la propagande de se propager sur Facebook”, précise le texte.

Le document détaille ensuite les techniques utilisées par l’entreprise pour détecter les contenus à scanner : marquage d’images interdites, traque de la récidive et des comptes organisés en réseau, compréhension algorithmique du langage pour repérer l’apologie du terrorisme, et collaboration avec les autres plateformes. En mai, le Guardian révélait des slides PowerPoint internes illustrant certaines de ces méthodes. C’est un changement de paradigme, puisqu’il s’agit non plus de sous-traiter à des humains une modération a posteriori sur la base d’un signalement, mais d’intervenir en amont, à l’aide d’un ordinateur. Cette automatisation est somme toute logique : elle suit la tendance des services de renseignement, qui tentent d’identifier les passages à l’acte potentiels à l’aide de “signaux faibles”, notamment numériques. Mais elle ne va pas sans poser de questions. Surtout quand on connaît le zèle et l’opacité de Facebook dans la censure de publications parfois légitimes.

Primo, cette mécanisation est-elle vraiment efficace ? YouTube a également annoncé son intention de recourir à l’IA pour purger les vidéos apologétiques du terrorisme. Problème : les aspirants djihadistes sont devenus des experts pour échapper à la vigilance des algorithmes. Ils ont développé des ruses de Sioux pour empêcher le retrait de leurs vidéos, qui vont du déréférencement volontaire à la suppression de la piste audio. Même avec des renforts humains - YouTube veut aller piocher des modérateurs dans le vivier des ONG -, cette guérilla numérique risque de durer.

Deuxio, qu’est-ce qu’un contenu terroriste ? Si les réseaux sociaux ont vocation à devenir des auxiliaires de police, si on délègue la régulation de la liberté d’expression aux machines, si on va jusqu’à leur offrir un véritable pouvoir de décision, on ne peut pas se contenter de plonger la tête dans le sable comme une vulgaire autruche. D’autant plus que cette interrogation est un boomerang qui passe son temps à nous revenir dans le visage. La loi antiterroriste du 13 novembre 2014 est venue entériner le blocage administratif des sites faisant l’apologie du terrorisme, sans passer par la case judiciaire. Oui, mais qu’est-ce qu’un contenu terroriste ? En juin 2016, la réforme de la procédure pénale a créé un délit de consultation de sites terroristes, censuré quelques mois plus tard par le Conseil constitutionnel (avant d'être rétabli à la hussarde pour être de nouveau attaqué par QPC dans la foulée). Parce qu’il ne définit pas - entre autres réserves - ce qu’est un contenu terroriste.

A la différence de notre appareil législatif, Facebook ne fait pas de lois. Facebook a des conditions générales d’utilisation, souveraines. Facebook utilise des algorithmes, discrétionnaires. On ne peut saisir aucune juridiction pour s’en plaindre. Evidemment, Facebook peut faire n’importe quoi avec notre vie privée et finir devant les tribunaux. Ca lui est déjà arrivé. Mais si Facebook doit devenir le sous-traitant de nos états d’urgence permanents, n’oublions pas de lui demander des comptes.