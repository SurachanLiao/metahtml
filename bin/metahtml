#!/usr/bin/python3

# process command line args
import argparse
import logging

parser = argparse.ArgumentParser(description='''
Downloads the specified URL and displays the metahtml properties extracted from the HTML.

WARNING:
On a Unix shell, you should wrap the URL in single quotation marks, as many of the characters commonly found in URLs have special meaning when used in the shell outside of quotation marks.
''')
parser.add_argument('url')
parser.add_argument('--simplify',action='store_true')
parser.add_argument('--text_only',action='store_true')
parser.add_argument('--cache_dir',default='/tmp/metahtml_cache')
parser.add_argument('--db')

parser_debug = parser.add_argument_group('debug arguments')
parser_debug.add_argument(
    '--debug',
    help='print lots of debugging statements',
    action='store_const', dest='loglevel', const=logging.DEBUG,
    default=logging.WARNING,
)
parser_debug.add_argument(
    '--verbose',
    help='print verbose status reports',
    action='store_const', dest='loglevel', const=logging.INFO,
)
args = parser.parse_args()    
logging.basicConfig(level=args.loglevel)

# the sys import is needed so that we can import from the current project
import sys
sys.path.append('.')
import metahtml

# load imports
import datetime
import hashlib
import json
import os
import requests
import sqlalchemy

# every url has a corresponding cache_file that is a hash of the url;
# the file is json formatted and contains both the timestamp the url was downloaded
# and the text of the download;
# if the file exists, then load the contents of the url from the cache;
# otherwise, download the url and store the url in the cache for future runs
cache_file = os.path.join(args.cache_dir, hashlib.md5(args.url.encode('utf-8')).hexdigest())
try:
    with open(cache_file) as f:
        cache = json.loads(f.read())
        accessed_at = cache['accessed_at']
        text = cache['text']
except FileNotFoundError:
    try:
        os.makedirs(args.cache_dir)
    except FileExistsError:
        pass
    logging.info('downloading url=',args.url)
    r = requests.get(args.url)
    text = r.text
    accessed_at = str(datetime.datetime.now(tz=datetime.timezone.utc))
    with open(cache_file,'w') as f:
        logging.info('saving contents to ',cache_file)
        f.write(json.dumps({
            'accessed_at' : accessed_at,
            'text' : text
            }))

# extract the meta
meta = metahtml.parse(text, args.url)
if args.simplify:
    meta = metahtml.simplify_meta(meta)
if args.text_only:
    print(meta['content']['text'])

# display the result
import json
output = json.dumps(
    dict(meta),
    indent=4,
    sort_keys=True,
    default=str
    )
print(output)

# add the results to the database
if args.db:
    engine = sqlalchemy.create_engine(args.db, connect_args={
        'application_name': 'metahtml',
        })  
    connection = engine.connect()
    sql = sqlalchemy.sql.text('''
        INSERT INTO metahtml.metahtml (url, accessed_at, id_source, jsonb) VALUES 
            (uri_normalize(:url), :accessed_at, :id_source, :jsonb);
        ''')
    res = connection.execute(sql,{
        'url' : args.url,
        'accessed_at' : accessed_at,
        'id_source' : -1, # this is a hardcoded constant in the sql for this file
        'jsonb' : output
        })

