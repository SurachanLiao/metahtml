#!/usr/bin/python3

# process command line args
import argparse
import logging

parser = argparse.ArgumentParser(description='''
Downloads the specified URL and displays the metahtml properties extracted from the HTML.

WARNING:
On a Unix shell, you should wrap the URL in single quotation marks, as many of the characters commonly found in URLs have special meaning when used in the shell outside of quotation marks.
''')
parser.add_argument('url')
parser.add_argument('--simplify',action='store_true')
parser.add_argument('--text_only',action='store_true')
parser.add_argument('--cache_dir',default='/tmp/metahtml_cache')
parser.add_argument('--db')

parser_debug = parser.add_argument_group('debug arguments')
parser_debug.add_argument(
    '--debug',
    help='print lots of debugging statements',
    action='store_const', dest='loglevel', const=logging.DEBUG,
    default=logging.WARNING,
)
parser_debug.add_argument(
    '--verbose',
    help='print verbose status reports',
    action='store_const', dest='loglevel', const=logging.INFO,
)
args = parser.parse_args()    
logging.basicConfig(level=args.loglevel)

# the sys import is needed so that we can import from the current project
import sys
sys.path.append('.')
import metahtml

# load imports
import hashlib
import json
import os
import requests
import sqlalchemy

# every url has a corresponding cache_file that is a hash of the url;
# if the file exists, then load the contents of the url from the cache;
# otherwise, download the url and store the url in the cache for future runs
cache_file = os.path.join(args.cache_dir, hashlib.md5(args.url.encode('utf-8')).hexdigest())
try:
    with open(cache_file) as f:
        text = f.read()
except FileNotFoundError:
    try:
        os.makedirs(args.cache_dir)
    except FileExistsError:
        pass
    logging.info('downloading url=',url)
    r = requests.get(args.url)
    text = r.text
    with open(cache_file,'w') as f:
        logging.info('saving contents to ',cache_file)
        f.write(text)

# extract the meta
meta = metahtml.parse(text, args.url)
if args.simplify:
    meta = metahtml.simplify_meta(meta)
if args.text_only:
    print(meta['content']['text'])

# display the result
import json
output = json.dumps(
    dict(meta),
    indent=4,
    sort_keys=True,
    default=str
    )
print(output)

if args.db:
    # create database connection
    engine = sqlalchemy.create_engine(args.db, connect_args={
        'application_name': 'metahtml',
        })  
    connection = engine.connect()

    sql = sqlalchemy.sql.text('''
        INSERT INTO metahtml.metahtml (jsonb) VALUES (:jsonb);
        ''')
    res = connection.execute(sql,{'jsonb':output})

